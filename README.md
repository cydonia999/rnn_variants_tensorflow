# RNN variants in Tensorflow

Implementation of variants of recurrent neural networks(GRU/LSTM) in Tensorflow.  

I implemented various kinds of recurrent neural networks in Tensorflow
in order to compare their accuracy with standard GRU and LSTM.

# RNN variants

- GRU variants by R. Jozefowicz et al. [1]
- LSTM variants by K. Greff et al. [2]
- LSTM variants by Y. Lu et al. [3] 

# Acknowledgments

- Code of Penn Tree Bank (PTB) language model is borrowed from Tensorflow models.
[link](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)

# References

1. Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever,
     An Empirical Exploration of Recurrent Network Architectures,
     Proceedings of The 32nd International Conference on Machine Learning, (2015).
     [link](http://jmlr.org/proceedings/papers/v37/jozefowicz15.html)

2. Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnik, Bas R. Steunebrink, Jurgen Schmidhuber,
     LSTM: A Search Space Odyssey,
     IEEE transactions on neural networks and learning systems, (2016).
     [arXiv](https://arxiv.org/abs/1503.04069)

3. Yuzhen Lu, Fathi M. Salem,
     Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural Networks, 
     arXiv preprint arXiv:1701.03441 (2017)
     [arXiv](https://arxiv.org/abs/1701.03441)

